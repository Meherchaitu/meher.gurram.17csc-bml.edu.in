{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML_Assignment10.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Meherchaitu/meher.gurram.17csc-bml.edu.in/blob/master/ML_Assignment10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-ije14bJbmH",
        "colab_type": "text"
      },
      "source": [
        "Problem Statement 1 : Say you are standing at the bottom of a staircase with  a dice. With each throw of the dice you either move down one step (if you get a 1 or 2 on the dice) or move up one step (if you get a 3, 4 or 5 on the dice). If you throw a 6 on the dice, you throw the dice again and move up the staircase by the number you get on that second throw. Note if you are on the base of the staircase you cannot move down! What is the probability that you will reach more than 60 steps after 250 throws of the dice. Change the code so that you have a function that takes as parameter, the number of throws\n",
        "Add a new parameter to the function that takes a probability distribution over all outcomes from a dice throw. For example (0.2,0.3,0.2,0.1,0.1,0.1) would suggest that the probability of getting a 1 is 0.2, 2 is 0.3 etc. How does that change the probability of reaching a step higher than 60?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGzwczIoYjkN",
        "colab_type": "code",
        "outputId": "9f37dada-9ff4-4153-e6cd-07db1851d9d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import style\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def roll_dice(prob=None):\n",
        "    if not prob:\n",
        "        return np.random.choice(a=[1,2,3,4,5,6])\n",
        "    else:\n",
        "        return np.random.choice(a=[1,2,3,4,5,6],p=prob)\n",
        "           \n",
        "def simulate(n=100,prob=None):\n",
        "    reached=0\n",
        "    for _ in range(n):\n",
        "        throws=250\n",
        "        curr_pos=0\n",
        "        while throws:\n",
        "            throws-=1\n",
        "            curr_step=roll_dice(prob)\n",
        "            if curr_pos>60:\n",
        "                reached+=1\n",
        "                break\n",
        "                \n",
        "            if curr_step in {1,2}:\n",
        "                curr_pos-=1\n",
        "            elif curr_step in {3,4,5}:\n",
        "                curr_pos+=1\n",
        "            else:\n",
        "                curr_step=roll_dice(prob)\n",
        "                curr_pos+=curr_step\n",
        "    print(reached/n)\n",
        "simulate() #100 iterations in without probability weights \n",
        "simulate(prob=[0.2,0.3,0.2,0.1,0.1,0.1]) #100 iterations with probability weights\n",
        "\n",
        "simulate(1000)\n",
        "simulate(1000,[0.2,0.3,0.2,0.1,0.1,0.1])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.0\n",
            "0.35\n",
            "1.0\n",
            "0.306\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SiW7kkuBJuv8",
        "colab_type": "text"
      },
      "source": [
        "Problem Statement 2 : . Generate random data for for Multiple Linear Regression, Logistic Regression, K-mean Clustering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLXx8PLaYld_",
        "colab_type": "code",
        "outputId": "e9a6e135-92e2-44bb-a6d1-555719c24cd6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#randomly generated data for the Multiple Linear regression \n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy\n",
        "import random\n",
        "from scipy.stats import norm\n",
        "random.seed(1)\n",
        "n_features = 4\n",
        "X = []\n",
        "for p in range(n_features):\n",
        "  X_p = scipy.stats.norm.rvs(0, 1, 100)\n",
        "  X.append(X_p)\n",
        "eps = scipy.stats.norm.rvs(0, 0.25,100)\n",
        "y = 1 + (0.5 * X[0]) + eps + (0.4 * X[1]) + (0.3 * X[2]) + (0.5 * X[3])\n",
        "data_mlr = {'X0': X[0],'X1':X[1],'X2':X[2],'X3':X[3],'Y': y }\n",
        "df = pd.DataFrame(data_mlr)\n",
        "print(df.head())\n",
        "print(df.tail())\n",
        "print(df.info())\n",
        "print(df.describe())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Random data for the Logistic regression\n",
        "n_features_of_the_model = 4\n",
        "X = []\n",
        "for i in range(n_features_of_the_model):\n",
        "  X_i = scipy.stats.norm.rvs(0, 1, 100)\n",
        "  X.append(X_i)\n",
        "a1 = (np.exp(1 + (0.5 * X[0]) + (0.4 * X[1]) + (0.3 * X[2]) + (0.5 * X[3]))/(1 + np.exp(1 + (0.5 * X[0]) + (0.4 * X[1]) + (0.3 * X[2]) + (0.5 * X[3]))))\n",
        "y1 = []\n",
        "for i in a1:\n",
        "  if (i>=0.5):\n",
        "    y1.append(1)\n",
        "  else:\n",
        "    y1.append(0)\n",
        "data_lr = {'X0': X[0],'X1':X[1],'X2':X[2],'X3':X[3],'Y': y1 }\n",
        "df1 = pd.DataFrame(data_lr)\n",
        "print(df.head())\n",
        "print(df.tail())\n",
        "print(df.info())\n",
        "print(df.describe())\n",
        "\n",
        "\n",
        "\n",
        "#Random data for K-mean clustering\n",
        "X_a= -2 * np.random.rand(100,2)\n",
        "X_b = 1 + 2 * np.random.rand(50,2)\n",
        "X_a[50:100, :] = X_b\n",
        "plt.scatter(X_a[ : , 0], X_a[ :, 1], s = 50)\n",
        "plt.show()\n",
        "data_kmeans = {'X0': X_a[:,0],'X1':X_a[:,1]}\n",
        "df3 = pd.DataFrame(data_kmeans)\n",
        "print(df.head())\n",
        "print(df.tail())\n",
        "print(df.info())\n",
        "print(df.describe())\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "         X0        X1        X2        X3         Y\n",
            "0  0.143191  1.105563  0.395973 -0.852665  0.897174\n",
            "1 -0.073775  1.666113  0.148978  0.678039  2.129374\n",
            "2  1.096578  0.209889 -0.141304  1.499918  2.383149\n",
            "3  2.990191 -0.086533 -0.223055  1.132949  3.193556\n",
            "4  0.546869  0.237080  0.781650  0.674217  2.007716\n",
            "          X0        X1        X2        X3         Y\n",
            "95  0.719435 -0.162917 -1.158265  0.031456  1.003493\n",
            "96  0.198369  1.125201 -0.915303 -0.565800  1.272624\n",
            "97 -0.627503  1.557492  0.305237 -0.549114  1.306809\n",
            "98 -0.753958  1.871396 -0.830697  0.064764  1.186805\n",
            "99  0.165842  0.672280  0.919613  0.755547  1.825624\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 100 entries, 0 to 99\n",
            "Data columns (total 5 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   X0      100 non-null    float64\n",
            " 1   X1      100 non-null    float64\n",
            " 2   X2      100 non-null    float64\n",
            " 3   X3      100 non-null    float64\n",
            " 4   Y       100 non-null    float64\n",
            "dtypes: float64(5)\n",
            "memory usage: 4.0 KB\n",
            "None\n",
            "               X0          X1          X2          X3           Y\n",
            "count  100.000000  100.000000  100.000000  100.000000  100.000000\n",
            "mean    -0.026592    0.123494   -0.061213   -0.091819    0.978461\n",
            "std      0.908692    1.062567    1.066641    0.929028    1.057197\n",
            "min     -2.221088   -2.390369   -3.052682   -2.337895   -1.900235\n",
            "25%     -0.681497   -0.597203   -0.564316   -0.715659    0.382880\n",
            "50%      0.095889    0.043462   -0.077156   -0.037881    1.015673\n",
            "75%      0.482328    0.758558    0.606961    0.596372    1.669475\n",
            "max      2.990191    2.901481    2.718623    2.118057    3.848416\n",
            "         X0        X1        X2        X3         Y\n",
            "0  0.143191  1.105563  0.395973 -0.852665  0.897174\n",
            "1 -0.073775  1.666113  0.148978  0.678039  2.129374\n",
            "2  1.096578  0.209889 -0.141304  1.499918  2.383149\n",
            "3  2.990191 -0.086533 -0.223055  1.132949  3.193556\n",
            "4  0.546869  0.237080  0.781650  0.674217  2.007716\n",
            "          X0        X1        X2        X3         Y\n",
            "95  0.719435 -0.162917 -1.158265  0.031456  1.003493\n",
            "96  0.198369  1.125201 -0.915303 -0.565800  1.272624\n",
            "97 -0.627503  1.557492  0.305237 -0.549114  1.306809\n",
            "98 -0.753958  1.871396 -0.830697  0.064764  1.186805\n",
            "99  0.165842  0.672280  0.919613  0.755547  1.825624\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 100 entries, 0 to 99\n",
            "Data columns (total 5 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   X0      100 non-null    float64\n",
            " 1   X1      100 non-null    float64\n",
            " 2   X2      100 non-null    float64\n",
            " 3   X3      100 non-null    float64\n",
            " 4   Y       100 non-null    float64\n",
            "dtypes: float64(5)\n",
            "memory usage: 4.0 KB\n",
            "None\n",
            "               X0          X1          X2          X3           Y\n",
            "count  100.000000  100.000000  100.000000  100.000000  100.000000\n",
            "mean    -0.026592    0.123494   -0.061213   -0.091819    0.978461\n",
            "std      0.908692    1.062567    1.066641    0.929028    1.057197\n",
            "min     -2.221088   -2.390369   -3.052682   -2.337895   -1.900235\n",
            "25%     -0.681497   -0.597203   -0.564316   -0.715659    0.382880\n",
            "50%      0.095889    0.043462   -0.077156   -0.037881    1.015673\n",
            "75%      0.482328    0.758558    0.606961    0.596372    1.669475\n",
            "max      2.990191    2.901481    2.718623    2.118057    3.848416\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "         X0        X1        X2        X3         Y\n",
            "0  0.143191  1.105563  0.395973 -0.852665  0.897174\n",
            "1 -0.073775  1.666113  0.148978  0.678039  2.129374\n",
            "2  1.096578  0.209889 -0.141304  1.499918  2.383149\n",
            "3  2.990191 -0.086533 -0.223055  1.132949  3.193556\n",
            "4  0.546869  0.237080  0.781650  0.674217  2.007716\n",
            "          X0        X1        X2        X3         Y\n",
            "95  0.719435 -0.162917 -1.158265  0.031456  1.003493\n",
            "96  0.198369  1.125201 -0.915303 -0.565800  1.272624\n",
            "97 -0.627503  1.557492  0.305237 -0.549114  1.306809\n",
            "98 -0.753958  1.871396 -0.830697  0.064764  1.186805\n",
            "99  0.165842  0.672280  0.919613  0.755547  1.825624\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 100 entries, 0 to 99\n",
            "Data columns (total 5 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   X0      100 non-null    float64\n",
            " 1   X1      100 non-null    float64\n",
            " 2   X2      100 non-null    float64\n",
            " 3   X3      100 non-null    float64\n",
            " 4   Y       100 non-null    float64\n",
            "dtypes: float64(5)\n",
            "memory usage: 4.0 KB\n",
            "None\n",
            "               X0          X1          X2          X3           Y\n",
            "count  100.000000  100.000000  100.000000  100.000000  100.000000\n",
            "mean    -0.026592    0.123494   -0.061213   -0.091819    0.978461\n",
            "std      0.908692    1.062567    1.066641    0.929028    1.057197\n",
            "min     -2.221088   -2.390369   -3.052682   -2.337895   -1.900235\n",
            "25%     -0.681497   -0.597203   -0.564316   -0.715659    0.382880\n",
            "50%      0.095889    0.043462   -0.077156   -0.037881    1.015673\n",
            "75%      0.482328    0.758558    0.606961    0.596372    1.669475\n",
            "max      2.990191    2.901481    2.718623    2.118057    3.848416\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0pQVHsrKLAB",
        "colab_type": "text"
      },
      "source": [
        "Problem 3: a)Linear regression using gradient descent                         \n",
        "                       b)Logistic regression using gradient descent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AuvSd0F2Yq6W",
        "colab_type": "code",
        "outputId": "9fdf29f9-ae04-46c7-f109-d40d0f0986d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 308
        }
      },
      "source": [
        "#linear regression using the gradient descent\n",
        "print(\" linear regrission using gradient descent are\")\n",
        "X = df.iloc[:,0].values\n",
        "y = df.iloc[:,4].values\n",
        "b1 = 0\n",
        "b0 = 0\n",
        "l = 0.001\n",
        "epochs = 100\n",
        " \n",
        "n = float(len(X))\n",
        "for i in range(epochs):\n",
        "  y_p = b1*X + b0\n",
        "  loss = np.sum(y_p - y1)**2\n",
        "  d1 = (-2/n) * sum(X * (y - y_p))\n",
        "  d0 = (-2/n) * sum(y - y_p)\n",
        "  b1 = b1 - (l*d1)\n",
        "  b0 = b0 - (l*d0)\n",
        "print(b1,b0)\n",
        "print()\n",
        "print()\n",
        "print()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#logistic regression using gradient descent\n",
        "print(\"from logistic using gradient descent are\")\n",
        "X1 = df1.iloc[:,0:4].values\n",
        "y1 = df1.iloc[:,4].values\n",
        "def sigmoid(Z):\n",
        "  return 1 /(1+np.exp(-Z))\n",
        "def loss(y1,y_hat):\n",
        "  return -np.mean(y1*np.log(y_hat) + (1-y1)*(np.log(1-y_hat)))\n",
        "W = np.zeros((4,1))\n",
        "b = np.zeros((1,1))\n",
        "m = len(y1)\n",
        "lr = 0.001\n",
        "for epoch in range(1000):\n",
        "  Z = np.matmul(X1,W)+b\n",
        "  A = sigmoid(Z)\n",
        "  logistic_loss = loss(y1,A)\n",
        "  dz = A - y1\n",
        "  dw = 1/m * np.matmul(X1.T,dz)\n",
        "  db = np.sum(dz)\n",
        "  W = W - lr*dw\n",
        "  b = b - lr*db\n",
        "  if epoch % 100 == 0:\n",
        "    print(logistic_loss)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " linear regrission using gradient descent are\n",
            "0.10784954990404076 0.1777982258186793\n",
            "\n",
            "\n",
            "\n",
            "from logistic using gradient descent are\n",
            "0.6931471805599453\n",
            "0.366167621226907\n",
            "0.36542347847421097\n",
            "0.36469244671176404\n",
            "0.36397441128639174\n",
            "0.36326925749615263\n",
            "0.36257687026656793\n",
            "0.3618971338425368\n",
            "0.36122993149835986\n",
            "0.3605751452680673\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yu4oNn-oLQHZ",
        "colab_type": "text"
      },
      "source": [
        "c)Linear Regression using L1 and L2 regularization                              \n",
        "d)Logistic regression using L1 and L2 regularization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mL2fLbxsYvVY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 690
        },
        "outputId": "128f3882-28a0-4906-8aea-949fe1d0d57d"
      },
      "source": [
        "print(\"Linear regression using L1 regularisation\")\n",
        "X = df.iloc[:,0].values\n",
        "y = df.iloc[:,4].values\n",
        "b1 = 0\n",
        "b0 = 0\n",
        "l = 0.001\n",
        "epochs = 100\n",
        "lam = 0.1\n",
        "n = float(len(X))\n",
        "for i in range(epochs):\n",
        "  y_p = b1*X + b0\n",
        "  loss = np.sum(y_p - y1)**2 + (lam * b1)\n",
        "  d1 = (-2/n) * sum(X * (y - y_p)) + lam\n",
        "  d0 = (-2/n) * sum(y - y_p)\n",
        "  b1 = b1 - (l*d1)\n",
        "  b0 = b0 - (l*d0)\n",
        "print(b1,b0)\n",
        "print()\n",
        "print()\n",
        "print()\n",
        "print()\n",
        "print(\"Linear regression using L2 regularisation\")\n",
        "X = df.iloc[:,0].values\n",
        "#print(X)\n",
        "y = df.iloc[:,4].values\n",
        "b1 = 0\n",
        "b0 = 0\n",
        "l = 0.001\n",
        "epochs = 100\n",
        "lam = 0.1\n",
        "n = float(len(X))\n",
        "for i in range(epochs):\n",
        "  y_p = b1*X + b0\n",
        "  loss = np.sum(y_p - y1)**2 + ((lam/2) * b1)\n",
        "  d1 = (-2/n) * sum(X * (y - y_p)) + (lam *b1)\n",
        "  d0 = (-2/n) * sum(y - y_p)\n",
        "  b1 = b1 - (l*d1)\n",
        "  b0 = b0 - (l*d0)\n",
        "print(b1,b0)\n",
        "print()\n",
        "print()\n",
        "print()\n",
        "print()\n",
        "print(\"Logistic regression using L1 regularisation\")\n",
        "X1 = df1.iloc[:,0:4].values\n",
        "y1 = df1.iloc[:,4].values\n",
        "lam = 0.1\n",
        "def sigmoid(Z):\n",
        "  return 1 /(1+np.exp(-Z))\n",
        "\n",
        "def loss(y1,y_hat):\n",
        "  return -np.mean(y1*np.log(y_hat) + (1-y1)*(np.log(1-y_hat))) + (lam * (np.sum(W)))\n",
        "\n",
        "W = np.zeros((4,1))\n",
        "b = np.zeros((1,1))\n",
        "\n",
        "m = len(y1)\n",
        "lr = 0.001\n",
        "for epoch in range(1000):\n",
        "  Z = np.matmul(X1,W)+b\n",
        "  A = sigmoid(Z)\n",
        "  logistic_loss = loss(y1,A)\n",
        "  dz = A - y1\n",
        "  dw = 1/m * np.matmul(X1.T,dz) + lam\n",
        "  db = np.sum(dz)\n",
        "\n",
        "  W = W - lr*dw\n",
        "  b = b - lr*db\n",
        "\n",
        "  if epoch % 100 == 0:\n",
        "    print(logistic_loss)\n",
        "\n",
        "print()\n",
        "print()\n",
        "print()\n",
        "print(\"Logistic regression using L2 regularisation\")\n",
        "X1 = df1.iloc[:,0:4].values\n",
        "y1 = df1.iloc[:,4].values\n",
        "lam = 0.1\n",
        "def sigmoid(Z):\n",
        "  return 1 /(1+np.exp(-Z))\n",
        "\n",
        "def loss(y1,y_hat):\n",
        "  return -np.mean(y1*np.log(y_hat) + (1-y1)*(np.log(1-y_hat))) + (lam * (np.sum(np.square(W))))\n",
        "\n",
        "W = np.zeros((4,1))\n",
        "b = np.zeros((1,1))\n",
        "\n",
        "m = len(y1)\n",
        "lr = 0.001\n",
        "for epoch in range(1000):\n",
        "  Z = np.matmul(X1,W)+b\n",
        "  A = sigmoid(Z)\n",
        "  logistic_loss = loss(y1,A)\n",
        "  dz = A - y1\n",
        "  dw = 1/m * np.matmul(X1.T,dz) + lam * W\n",
        "  db = np.sum(dz)\n",
        "\n",
        "  W = W - lr*dw\n",
        "  b = b - lr*db\n",
        "\n",
        "  if epoch % 100 == 0:\n",
        "    print(logistic_loss)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Linear regression using L1 regularisation\n",
            "0.09861786712305266 0.17777482874665992\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Linear regression using L2 regularisation\n",
            "0.10733195038888367 0.17779734528763144\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Logistic regression using L1 regularisation\n",
            "0.6931471805599453\n",
            "-0.03151935876449319\n",
            "-0.4264466203863361\n",
            "-0.8175346218862425\n",
            "-1.2048375867143253\n",
            "-1.588409888662951\n",
            "-1.9683059768667726\n",
            "-2.344580306204235\n",
            "-2.717287272761595\n",
            "-3.086481154023287\n",
            "\n",
            "\n",
            "\n",
            "Logistic regression using L2 regularisation\n",
            "0.6931471805599453\n",
            "0.3669213190386327\n",
            "0.36838235032027195\n",
            "0.37122651796255113\n",
            "0.37537532218106945\n",
            "0.3807532642003709\n",
            "0.38728775661119136\n",
            "0.39490903742948763\n",
            "0.4035500874959254\n",
            "0.4131465508775128\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6FqHJruLrjV",
        "colab_type": "text"
      },
      "source": [
        "e)K-means Clustering "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0RRkhDBZY7MY",
        "colab_type": "code",
        "outputId": "eb03a58d-6956-4c7f-e6d5-30e928f69bc3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267
        }
      },
      "source": [
        "class K_Means:\n",
        "    def __init__(self, k=2, tol=0.001, max_iter=300):\n",
        "        self.k = k\n",
        "        self.tol = tol\n",
        "        self.max_iter = max_iter\n",
        "\n",
        "    def fit(self,data):\n",
        "\n",
        "        self.centroids = {}\n",
        "\n",
        "        for i in range(self.k):\n",
        "            self.centroids[i] = data[i]\n",
        "\n",
        "        for i in range(self.max_iter):\n",
        "            self.classifications = {}\n",
        "\n",
        "            for i in range(self.k):\n",
        "                self.classifications[i] = []\n",
        "\n",
        "            for featureset in X:\n",
        "                distances = [np.linalg.norm(featureset-self.centroids[centroid]) for centroid in self.centroids]\n",
        "                classification = distances.index(min(distances))\n",
        "                self.classifications[classification].append(featureset)\n",
        "\n",
        "            prev_centroids = dict(self.centroids)\n",
        "\n",
        "            for classification in self.classifications:\n",
        "                self.centroids[classification] = np.average(self.classifications[classification],axis=0)\n",
        "\n",
        "            optimized = True\n",
        "\n",
        "            for c in self.centroids:\n",
        "                original_centroid = prev_centroids[c]\n",
        "                current_centroid = self.centroids[c]\n",
        "                if np.sum((current_centroid-original_centroid)/original_centroid*100.0) > self.tol:\n",
        "                    print(np.sum((current_centroid-original_centroid)/original_centroid*100.0))\n",
        "                    optimized = False\n",
        "\n",
        "            if optimized:\n",
        "                break\n",
        "\n",
        "    def predict(self,data):\n",
        "        distances = [np.linalg.norm(data-self.centroids[centroid]) for centroid in self.centroids]\n",
        "        classification = distances.index(min(distances))\n",
        "        return classification\n",
        "        \n",
        "colors = 10*[\"g\",\"r\",\"c\",\"b\",\"k\"]\n",
        "X = df3.iloc[:,0:2].values\n",
        "clf = K_Means()\n",
        "clf.fit(X)\n",
        "\n",
        "for centroid in clf.centroids:\n",
        "    plt.scatter(clf.centroids[centroid][0], clf.centroids[centroid][1],\n",
        "                marker=\"o\", color=\"k\", s=150, linewidths=5)\n",
        "\n",
        "for classification in clf.classifications:\n",
        "    color = colors[classification]\n",
        "    for featureset in clf.classifications[classification]:\n",
        "        plt.scatter(featureset[0], featureset[1], marker=\"x\", color=color, s=150, linewidths=5)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AaBVxF6RL4CU",
        "colab_type": "text"
      },
      "source": [
        "Problem 4 :Linear Regression from scratch using OOPS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MBIqfDuZAKJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        },
        "outputId": "6961780d-39cc-4747-bdaf-07b628b59902"
      },
      "source": [
        "\n",
        "import numpy as np\n",
        "\n",
        "class LinearRegressionModel():\n",
        "\n",
        "    def __init__(self, dataset, learning_rate, num_iterations):\n",
        "        self.dataset = np.array(dataset)\n",
        "        self.b = 0  \n",
        "        self.m = 0  \n",
        "        self.learning_rate = learning_rate\n",
        "        self.num_iterations = num_iterations\n",
        "        self.M = len(self.dataset)\n",
        "        self.tota  l_error = 0\n",
        "\n",
        "    def apply_gradient_descent(self):\n",
        "        for i in range(self.num_iterations):\n",
        "            self.do_gradient_step()\n",
        "\n",
        "    def do_gradient_step(self):\n",
        "        b_summation = 0\n",
        "        m_summation = 0\n",
        "        for i in range(self.M):\n",
        "            x_value = self.dataset[i, 0]\n",
        "            y_value = self.dataset[i, 1]\n",
        "            b_summation += (((self.m * x_value) + self.b) - y_value) \n",
        "            m_summation += (((self.m * x_value) + self.b) - y_value) * x_value\n",
        "        self.b = self.b - (self.learning_rate * (1/self.M) * b_summation)\n",
        "        self.m = self.m - (self.learning_rate * (1/self.M) * m_summation)\n",
        "      \n",
        "    def compute_error(self):\n",
        "        for i in range(self.M):\n",
        "            x_value = self.dataset[i, 0]\n",
        "            y_value = self.dataset[i, 1]\n",
        "            self.total_error += ((self.m * x_value) + self.b) - y_value\n",
        "        return self.total_error\n",
        "\n",
        "    def __str__(self):\n",
        "        return \"Results: b: {}, m: {}, Final Total error: {}\".format(round(self.b, 2), round(self.m, 2), round(self.compute_error(), 2))\n",
        "\n",
        "    def get_prediction_based_on(self, x):\n",
        "        return round(float((self.m * x) + self.b), 2) # Type: Numpy float.\n",
        "\n",
        "def main():\n",
        "    school_dataset = np.genfromtxt(DATASET_PATH, delimiter=\",\")\n",
        "    lr = LinearRegressionModel(school_dataset, 0.0001, 1000)\n",
        "    lr.apply_gradient_descent()\n",
        "    hours = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
        "    for hour in hours:\n",
        "        print(\"Studied {} hours and got {} points.\".format(hour, lr.get_prediction_based_on(hour)))\n",
        "    print(lr)\n",
        "\n",
        "if __name__ == \"__main__\": main()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-0beb326c47a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-14-0beb326c47a5>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0mschool_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenfromtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATASET_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m     \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLinearRegressionModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschool_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mlr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradient_descent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'DATASET_PATH' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2AiZS-eMFRR",
        "colab_type": "text"
      },
      "source": [
        "Logistic Regression using oops"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37AURcbRZBcQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LogisticRegression:\n",
        "  def __init__(self, learning_rate, num_iters, fit_intercept = True, verbose = False):\n",
        "    self.learning_rate = learning_rate\n",
        "    self.num_iters = num_iters\n",
        "    self.fit_intercept = fit_intercept\n",
        "    self.verbose = verbose\n",
        "  def __add_intercept(self, X):\n",
        "    intercept = np.ones((X.shape[0],1))\n",
        "    return np.concatenate((intercept,X),axis=1)\n",
        "  def __sigmoid(self,z):\n",
        "    return 1/(1+np.exp(-z))\n",
        "  def __loss(self, h, y):\n",
        "    return (-y * np.log(h) - (1-y) * np.log(1-h)).mean()\n",
        "  \n",
        "  def fit(self,X,y):\n",
        "    if self.fit_intercept:\n",
        "      X = self.__add_intercept(X)\n",
        "    self.theta = np.zeros(X.shape[1])\n",
        "    \n",
        "    for i in range(self.num_iters):\n",
        "      z = np.dot(X,self.theta)\n",
        "      h = self.__sigmoid(z)\n",
        "      gradient = np.dot(X.T,(h-y))/y.size\n",
        "      \n",
        "      self.theta -= self.learning_rate * gradient\n",
        "      \n",
        "      z = np.dot(X,self.theta)\n",
        "      h = self.__sigmoid(z)\n",
        "      loss = self.__loss(h,y)\n",
        "      \n",
        "      if self.verbose == True and i % 1000 == 0:\n",
        "        print(f'Loss: {loss}\\t')\n",
        "  def predict_probability(self,X):\n",
        "    if self.fit_intercept:\n",
        "      X = self.__add_intercept(X)\n",
        "    return self.__sigmoid(np.dot(X,self.theta))\n",
        "  def predict(self,X):\n",
        "    return (self.predict_probability(X).round())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GlW7KyNOMAcy",
        "colab_type": "text"
      },
      "source": [
        "K means from scratch using oops"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODgvm-8nZFVo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class K_Means:\n",
        "    def __init__(self, k=2, tol=0.001, max_iter=300):\n",
        "        self.k = k\n",
        "        self.tol = tol\n",
        "        self.max_iter = max_iter\n",
        "\n",
        "    def fit(self,data):\n",
        "\n",
        "        self.centroids = {}\n",
        "\n",
        "        for i in range(self.k):\n",
        "            self.centroids[i] = data[i]\n",
        "\n",
        "        for i in range(self.max_iter):\n",
        "            self.classifications = {}\n",
        "\n",
        "            for i in range(self.k):\n",
        "                self.classifications[i] = []\n",
        "\n",
        "            for featureset in X:\n",
        "                distances = [np.linalg.norm(featureset-self.centroids[centroid]) for centroid in self.centroids]\n",
        "                classification = distances.index(min(distances))\n",
        "                self.classifications[classification].append(featureset)\n",
        "\n",
        "            prev_centroids = dict(self.centroids)\n",
        "\n",
        "            for classification in self.classifications:\n",
        "                self.centroids[classification] = np.average(self.classifications[classification],axis=0)\n",
        "\n",
        "            optimized = True\n",
        "\n",
        "            for c in self.centroids:\n",
        "                original_centroid = prev_centroids[c]\n",
        "                current_centroid = self.centroids[c]\n",
        "                if np.sum((current_centroid-original_centroid)/original_centroid*100.0) > self.tol:\n",
        "                    print(np.sum((current_centroid-original_centroid)/original_centroid*100.0))\n",
        "                    optimized = False\n",
        "\n",
        "            if optimized:\n",
        "                break\n",
        "\n",
        "    def predict(self,data):\n",
        "        distances = [np.linalg.norm(data-self.centroids[centroid]) for centroid in self.centroids]\n",
        "        classification = distances.index(min(distances))\n",
        "        return classification\n",
        "        \n",
        "colors = 10*[\"g\",\"r\",\"c\",\"b\",\"k\"]"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}