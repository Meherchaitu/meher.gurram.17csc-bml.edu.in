{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Meher_Gurram_1700148C202.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Meherchaitu/meher.gurram.17csc-bml.edu.in/blob/master/Meher_Gurram_1700148C202.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-ije14bJbmH",
        "colab_type": "text"
      },
      "source": [
        "Problem Statement 1 : Say you are standing at the bottom of a staircase with  a dice. With each throw of the dice you either move down one step (if you get a 1 or 2 on the dice) or move up one step (if you get a 3, 4 or 5 on the dice). If you throw a 6 on the dice, you throw the dice again and move up the staircase by the number you get on that second throw. Note if you are on the base of the staircase you cannot move down! What is the probability that you will reach more than 60 steps after 250 throws of the dice. Change the code so that you have a function that takes as parameter, the number of throws\n",
        "Add a new parameter to the function that takes a probability distribution over all outcomes from a dice throw. For example (0.2,0.3,0.2,0.1,0.1,0.1) would suggest that the probability of getting a 1 is 0.2, 2 is 0.3 etc. How does that change the probability of reaching a step higher than 60?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGzwczIoYjkN",
        "colab_type": "code",
        "outputId": "9f37dada-9ff4-4153-e6cd-07db1851d9d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import style\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def roll_dice(prob=None):\n",
        "    if not prob:\n",
        "        return np.random.choice(a=[1,2,3,4,5,6])\n",
        "    else:\n",
        "        return np.random.choice(a=[1,2,3,4,5,6],p=prob)\n",
        "           \n",
        "def simulate(n=100,prob=None):\n",
        "    reached=0\n",
        "    for _ in range(n):\n",
        "        throws=250\n",
        "        curr_pos=0\n",
        "        while throws:\n",
        "            throws-=1\n",
        "            curr_step=roll_dice(prob)\n",
        "            if curr_pos>60:\n",
        "                reached+=1\n",
        "                break\n",
        "                \n",
        "            if curr_step in {1,2}:\n",
        "                curr_pos-=1\n",
        "            elif curr_step in {3,4,5}:\n",
        "                curr_pos+=1\n",
        "            else:\n",
        "                curr_step=roll_dice(prob)\n",
        "                curr_pos+=curr_step\n",
        "    print(reached/n)\n",
        "simulate() #100 iterations in without probability weights \n",
        "simulate(prob=[0.2,0.3,0.2,0.1,0.1,0.1]) #100 iterations with probability weights\n",
        "\n",
        "simulate(1000)\n",
        "simulate(1000,[0.2,0.3,0.2,0.1,0.1,0.1])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.0\n",
            "0.35\n",
            "1.0\n",
            "0.306\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SiW7kkuBJuv8",
        "colab_type": "text"
      },
      "source": [
        "Problem Statement 2 : . Generate random data for for Multiple Linear Regression, Logistic Regression, K-mean Clustering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLXx8PLaYld_",
        "colab_type": "code",
        "outputId": "a1154fa1-1eac-49d9-a3f5-9275ea8182f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#randomly generated data for the Multiple Linear regression \n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy\n",
        "import random\n",
        "from scipy.stats import norm\n",
        "random.seed(1)\n",
        "n_features = 4\n",
        "X = []\n",
        "for p in range(n_features):\n",
        "  X_p = scipy.stats.norm.rvs(0, 1, 100)\n",
        "  X.append(X_p)\n",
        "eps = scipy.stats.norm.rvs(0, 0.25,100)\n",
        "y = 1 + (0.5 * X[0]) + eps + (0.4 * X[1]) + (0.3 * X[2]) + (0.5 * X[3])\n",
        "data_mlr = {'X0': X[0],'X1':X[1],'X2':X[2],'X3':X[3],'Y': y }\n",
        "df = pd.DataFrame(data_mlr)\n",
        "print(df.head())\n",
        "print(df.tail())\n",
        "print(df.info())\n",
        "print(df.describe())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Random data for the Logistic regression\n",
        "n_features_of_the_model = 4\n",
        "X = []\n",
        "for i in range(n_features_of_the_model):\n",
        "  X_i = scipy.stats.norm.rvs(0, 1, 100)\n",
        "  X.append(X_i)\n",
        "a1 = (np.exp(1 + (0.5 * X[0]) + (0.4 * X[1]) + (0.3 * X[2]) + (0.5 * X[3]))/(1 + np.exp(1 + (0.5 * X[0]) + (0.4 * X[1]) + (0.3 * X[2]) + (0.5 * X[3]))))\n",
        "y1 = []\n",
        "for i in a1:\n",
        "  if (i>=0.5):\n",
        "    y1.append(1)\n",
        "  else:\n",
        "    y1.append(0)\n",
        "data_lr = {'X0': X[0],'X1':X[1],'X2':X[2],'X3':X[3],'Y': y1 }\n",
        "df1 = pd.DataFrame(data_lr)\n",
        "print(df.head())\n",
        "print(df.tail())\n",
        "print(df.info())\n",
        "print(df.describe())\n",
        "\n",
        "\n",
        "\n",
        "#Random data for K-mean clustering\n",
        "X_a= -2 * np.random.rand(100,2)\n",
        "X_b = 1 + 2 * np.random.rand(50,2)\n",
        "X_a[50:100, :] = X_b\n",
        "plt.scatter(X_a[ : , 0], X_a[ :, 1], s = 50)\n",
        "plt.show()\n",
        "data_kmeans = {'X0': X_a[:,0],'X1':X_a[:,1]}\n",
        "df3 = pd.DataFrame(data_kmeans)\n",
        "print(df.head())\n",
        "print(df.tail())\n",
        "print(df.info())\n",
        "print(df.describe())\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "         X0        X1        X2        X3         Y\n",
            "0  1.913198  0.432149  1.162743 -1.272417  2.144764\n",
            "1  0.861609  1.578357  2.034861  0.444997  2.953343\n",
            "2 -0.324511  2.287331 -0.550245  0.802964  1.904276\n",
            "3  1.290413 -0.465784  0.342253  0.622516  1.993885\n",
            "4 -0.763167 -1.464778 -2.849127  0.862145 -0.116758\n",
            "          X0        X1        X2        X3         Y\n",
            "95 -0.264010  0.893585 -0.562609 -1.443555  0.388658\n",
            "96 -0.392279 -0.603496 -0.859629  0.130394  0.486905\n",
            "97 -1.735744 -0.398849 -1.748754  0.418384 -0.526404\n",
            "98  1.824088 -0.230701 -1.326670  0.908927  1.746331\n",
            "99 -0.572169  0.878557  1.044176 -1.769469  0.540633\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 100 entries, 0 to 99\n",
            "Data columns (total 5 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   X0      100 non-null    float64\n",
            " 1   X1      100 non-null    float64\n",
            " 2   X2      100 non-null    float64\n",
            " 3   X3      100 non-null    float64\n",
            " 4   Y       100 non-null    float64\n",
            "dtypes: float64(5)\n",
            "memory usage: 4.0 KB\n",
            "None\n",
            "               X0          X1          X2          X3           Y\n",
            "count  100.000000  100.000000  100.000000  100.000000  100.000000\n",
            "mean    -0.082706    0.060492    0.056722    0.064184    1.047178\n",
            "std      1.067709    0.972680    0.958138    0.956687    0.874773\n",
            "min     -1.856183   -1.950787   -2.849127   -2.200366   -1.050521\n",
            "25%     -0.787811   -0.591957   -0.520507   -0.588278    0.452164\n",
            "50%     -0.109511    0.123262    0.274539    0.126415    0.912356\n",
            "75%      0.636048    0.671506    0.749968    0.597479    1.623615\n",
            "max      2.267893    2.287331    2.034861    3.077151    3.540746\n",
            "         X0        X1        X2        X3         Y\n",
            "0  1.913198  0.432149  1.162743 -1.272417  2.144764\n",
            "1  0.861609  1.578357  2.034861  0.444997  2.953343\n",
            "2 -0.324511  2.287331 -0.550245  0.802964  1.904276\n",
            "3  1.290413 -0.465784  0.342253  0.622516  1.993885\n",
            "4 -0.763167 -1.464778 -2.849127  0.862145 -0.116758\n",
            "          X0        X1        X2        X3         Y\n",
            "95 -0.264010  0.893585 -0.562609 -1.443555  0.388658\n",
            "96 -0.392279 -0.603496 -0.859629  0.130394  0.486905\n",
            "97 -1.735744 -0.398849 -1.748754  0.418384 -0.526404\n",
            "98  1.824088 -0.230701 -1.326670  0.908927  1.746331\n",
            "99 -0.572169  0.878557  1.044176 -1.769469  0.540633\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 100 entries, 0 to 99\n",
            "Data columns (total 5 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   X0      100 non-null    float64\n",
            " 1   X1      100 non-null    float64\n",
            " 2   X2      100 non-null    float64\n",
            " 3   X3      100 non-null    float64\n",
            " 4   Y       100 non-null    float64\n",
            "dtypes: float64(5)\n",
            "memory usage: 4.0 KB\n",
            "None\n",
            "               X0          X1          X2          X3           Y\n",
            "count  100.000000  100.000000  100.000000  100.000000  100.000000\n",
            "mean    -0.082706    0.060492    0.056722    0.064184    1.047178\n",
            "std      1.067709    0.972680    0.958138    0.956687    0.874773\n",
            "min     -1.856183   -1.950787   -2.849127   -2.200366   -1.050521\n",
            "25%     -0.787811   -0.591957   -0.520507   -0.588278    0.452164\n",
            "50%     -0.109511    0.123262    0.274539    0.126415    0.912356\n",
            "75%      0.636048    0.671506    0.749968    0.597479    1.623615\n",
            "max      2.267893    2.287331    2.034861    3.077151    3.540746\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "         X0        X1        X2        X3         Y\n",
            "0  1.913198  0.432149  1.162743 -1.272417  2.144764\n",
            "1  0.861609  1.578357  2.034861  0.444997  2.953343\n",
            "2 -0.324511  2.287331 -0.550245  0.802964  1.904276\n",
            "3  1.290413 -0.465784  0.342253  0.622516  1.993885\n",
            "4 -0.763167 -1.464778 -2.849127  0.862145 -0.116758\n",
            "          X0        X1        X2        X3         Y\n",
            "95 -0.264010  0.893585 -0.562609 -1.443555  0.388658\n",
            "96 -0.392279 -0.603496 -0.859629  0.130394  0.486905\n",
            "97 -1.735744 -0.398849 -1.748754  0.418384 -0.526404\n",
            "98  1.824088 -0.230701 -1.326670  0.908927  1.746331\n",
            "99 -0.572169  0.878557  1.044176 -1.769469  0.540633\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 100 entries, 0 to 99\n",
            "Data columns (total 5 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   X0      100 non-null    float64\n",
            " 1   X1      100 non-null    float64\n",
            " 2   X2      100 non-null    float64\n",
            " 3   X3      100 non-null    float64\n",
            " 4   Y       100 non-null    float64\n",
            "dtypes: float64(5)\n",
            "memory usage: 4.0 KB\n",
            "None\n",
            "               X0          X1          X2          X3           Y\n",
            "count  100.000000  100.000000  100.000000  100.000000  100.000000\n",
            "mean    -0.082706    0.060492    0.056722    0.064184    1.047178\n",
            "std      1.067709    0.972680    0.958138    0.956687    0.874773\n",
            "min     -1.856183   -1.950787   -2.849127   -2.200366   -1.050521\n",
            "25%     -0.787811   -0.591957   -0.520507   -0.588278    0.452164\n",
            "50%     -0.109511    0.123262    0.274539    0.126415    0.912356\n",
            "75%      0.636048    0.671506    0.749968    0.597479    1.623615\n",
            "max      2.267893    2.287331    2.034861    3.077151    3.540746\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0pQVHsrKLAB",
        "colab_type": "text"
      },
      "source": [
        "Problem 3: a)Linear regression using gradient descent                         \n",
        "                       b)Logistic regression using gradient descent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AuvSd0F2Yq6W",
        "colab_type": "code",
        "outputId": "9fdf29f9-ae04-46c7-f109-d40d0f0986d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "#linear regression using the gradient descent\n",
        "print(\" linear regrission using gradient descent are\")\n",
        "X = df.iloc[:,0].values\n",
        "y = df.iloc[:,4].values\n",
        "b1 = 0\n",
        "b0 = 0\n",
        "l = 0.001\n",
        "epochs = 100\n",
        " \n",
        "n = float(len(X))\n",
        "for i in range(epochs):\n",
        "  y_p = b1*X + b0\n",
        "  loss = np.sum(y_p - y1)**2\n",
        "  d1 = (-2/n) * sum(X * (y - y_p))\n",
        "  d0 = (-2/n) * sum(y - y_p)\n",
        "  b1 = b1 - (l*d1)\n",
        "  b0 = b0 - (l*d0)\n",
        "print(b1,b0)\n",
        "print()\n",
        "print()\n",
        "print()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#logistic regression using gradient descent\n",
        "print(\"from logistic using gradient descent are\")\n",
        "X1 = df1.iloc[:,0:4].values\n",
        "y1 = df1.iloc[:,4].values\n",
        "def sigmoid(Z):\n",
        "  return 1 /(1+np.exp(-Z))\n",
        "def loss(y1,y_hat):\n",
        "  return -np.mean(y1*np.log(y_hat) + (1-y1)*(np.log(1-y_hat)))\n",
        "W = np.zeros((4,1))\n",
        "b = np.zeros((1,1))\n",
        "m = len(y1)\n",
        "lr = 0.001\n",
        "for epoch in range(1000):\n",
        "  Z = np.matmul(X1,W)+b\n",
        "  A = sigmoid(Z)\n",
        "  logistic_loss = loss(y1,A)\n",
        "  dz = A - y1\n",
        "  dw = 1/m * np.matmul(X1.T,dz)\n",
        "  db = np.sum(dz)\n",
        "  W = W - lr*dw\n",
        "  b = b - lr*db\n",
        "  if epoch % 100 == 0:\n",
        "    print(logistic_loss)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " linear regrission using gradient descent are\n",
            "0.10784954990404076 0.1777982258186793\n",
            "\n",
            "\n",
            "\n",
            "from logistic using gradient descent are\n",
            "0.6931471805599453\n",
            "0.366167621226907\n",
            "0.36542347847421097\n",
            "0.36469244671176404\n",
            "0.36397441128639174\n",
            "0.36326925749615263\n",
            "0.36257687026656793\n",
            "0.3618971338425368\n",
            "0.36122993149835986\n",
            "0.3605751452680673\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yu4oNn-oLQHZ",
        "colab_type": "text"
      },
      "source": [
        "c)Linear Regression using L1 and L2 regularization                              \n",
        "d)Logistic regression using L1 and L2 regularization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mL2fLbxsYvVY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 638
        },
        "outputId": "128f3882-28a0-4906-8aea-949fe1d0d57d"
      },
      "source": [
        "print(\"Linear regression using L1 regularisation\")\n",
        "X = df.iloc[:,0].values\n",
        "y = df.iloc[:,4].values\n",
        "b1 = 0\n",
        "b0 = 0\n",
        "l = 0.001\n",
        "epochs = 100\n",
        "lam = 0.1\n",
        "n = float(len(X))\n",
        "for i in range(epochs):\n",
        "  y_p = b1*X + b0\n",
        "  loss = np.sum(y_p - y1)**2 + (lam * b1)\n",
        "  d1 = (-2/n) * sum(X * (y - y_p)) + lam\n",
        "  d0 = (-2/n) * sum(y - y_p)\n",
        "  b1 = b1 - (l*d1)\n",
        "  b0 = b0 - (l*d0)\n",
        "print(b1,b0)\n",
        "print()\n",
        "print()\n",
        "print()\n",
        "print()\n",
        "print(\"Linear regression using L2 regularisation\")\n",
        "X = df.iloc[:,0].values\n",
        "#print(X)\n",
        "y = df.iloc[:,4].values\n",
        "b1 = 0\n",
        "b0 = 0\n",
        "l = 0.001\n",
        "epochs = 100\n",
        "lam = 0.1\n",
        "n = float(len(X))\n",
        "for i in range(epochs):\n",
        "  y_p = b1*X + b0\n",
        "  loss = np.sum(y_p - y1)**2 + ((lam/2) * b1)\n",
        "  d1 = (-2/n) * sum(X * (y - y_p)) + (lam *b1)\n",
        "  d0 = (-2/n) * sum(y - y_p)\n",
        "  b1 = b1 - (l*d1)\n",
        "  b0 = b0 - (l*d0)\n",
        "print(b1,b0)\n",
        "print()\n",
        "print()\n",
        "print()\n",
        "print()\n",
        "print(\"Logistic regression using L1 regularisation\")\n",
        "X1 = df1.iloc[:,0:4].values\n",
        "y1 = df1.iloc[:,4].values\n",
        "lam = 0.1\n",
        "def sigmoid(Z):\n",
        "  return 1 /(1+np.exp(-Z))\n",
        "\n",
        "def loss(y1,y_hat):\n",
        "  return -np.mean(y1*np.log(y_hat) + (1-y1)*(np.log(1-y_hat))) + (lam * (np.sum(W)))\n",
        "\n",
        "W = np.zeros((4,1))\n",
        "b = np.zeros((1,1))\n",
        "\n",
        "m = len(y1)\n",
        "lr = 0.001\n",
        "for epoch in range(1000):\n",
        "  Z = np.matmul(X1,W)+b\n",
        "  A = sigmoid(Z)\n",
        "  logistic_loss = loss(y1,A)\n",
        "  dz = A - y1\n",
        "  dw = 1/m * np.matmul(X1.T,dz) + lam\n",
        "  db = np.sum(dz)\n",
        "\n",
        "  W = W - lr*dw\n",
        "  b = b - lr*db\n",
        "\n",
        "  if epoch % 100 == 0:\n",
        "    print(logistic_loss)\n",
        "\n",
        "print()\n",
        "print()\n",
        "print()\n",
        "print(\"Logistic regression using L2 regularisation\")\n",
        "X1 = df1.iloc[:,0:4].values\n",
        "y1 = df1.iloc[:,4].values\n",
        "lam = 0.1\n",
        "def sigmoid(Z):\n",
        "  return 1 /(1+np.exp(-Z))\n",
        "\n",
        "def loss(y1,y_hat):\n",
        "  return -np.mean(y1*np.log(y_hat) + (1-y1)*(np.log(1-y_hat))) + (lam * (np.sum(np.square(W))))\n",
        "\n",
        "W = np.zeros((4,1))\n",
        "b = np.zeros((1,1))\n",
        "\n",
        "m = len(y1)\n",
        "lr = 0.001\n",
        "for epoch in range(1000):\n",
        "  Z = np.matmul(X1,W)+b\n",
        "  A = sigmoid(Z)\n",
        "  logistic_loss = loss(y1,A)\n",
        "  dz = A - y1\n",
        "  dw = 1/m * np.matmul(X1.T,dz) + lam * W\n",
        "  db = np.sum(dz)\n",
        "\n",
        "  W = W - lr*dw\n",
        "  b = b - lr*db\n",
        "\n",
        "  if epoch % 100 == 0:\n",
        "    print(logistic_loss)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Linear regression using L1 regularisation\n",
            "0.09861786712305266 0.17777482874665992\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Linear regression using L2 regularisation\n",
            "0.10733195038888367 0.17779734528763144\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Logistic regression using L1 regularisation\n",
            "0.6931471805599453\n",
            "-0.03151935876449319\n",
            "-0.4264466203863361\n",
            "-0.8175346218862425\n",
            "-1.2048375867143253\n",
            "-1.588409888662951\n",
            "-1.9683059768667726\n",
            "-2.344580306204235\n",
            "-2.717287272761595\n",
            "-3.086481154023287\n",
            "\n",
            "\n",
            "\n",
            "Logistic regression using L2 regularisation\n",
            "0.6931471805599453\n",
            "0.3669213190386327\n",
            "0.36838235032027195\n",
            "0.37122651796255113\n",
            "0.37537532218106945\n",
            "0.3807532642003709\n",
            "0.38728775661119136\n",
            "0.39490903742948763\n",
            "0.4035500874959254\n",
            "0.4131465508775128\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6FqHJruLrjV",
        "colab_type": "text"
      },
      "source": [
        "e)K-means Clustering "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0RRkhDBZY7MY",
        "colab_type": "code",
        "outputId": "eb03a58d-6956-4c7f-e6d5-30e928f69bc3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267
        }
      },
      "source": [
        "class K_Means:\n",
        "    def __init__(self, k=2, tol=0.001, max_iter=300):\n",
        "        self.k = k\n",
        "        self.tol = tol\n",
        "        self.max_iter = max_iter\n",
        "\n",
        "    def fit(self,data):\n",
        "\n",
        "        self.centroids = {}\n",
        "\n",
        "        for i in range(self.k):\n",
        "            self.centroids[i] = data[i]\n",
        "\n",
        "        for i in range(self.max_iter):\n",
        "            self.classifications = {}\n",
        "\n",
        "            for i in range(self.k):\n",
        "                self.classifications[i] = []\n",
        "\n",
        "            for featureset in X:\n",
        "                distances = [np.linalg.norm(featureset-self.centroids[centroid]) for centroid in self.centroids]\n",
        "                classification = distances.index(min(distances))\n",
        "                self.classifications[classification].append(featureset)\n",
        "\n",
        "            prev_centroids = dict(self.centroids)\n",
        "\n",
        "            for classification in self.classifications:\n",
        "                self.centroids[classification] = np.average(self.classifications[classification],axis=0)\n",
        "\n",
        "            optimized = True\n",
        "\n",
        "            for c in self.centroids:\n",
        "                original_centroid = prev_centroids[c]\n",
        "                current_centroid = self.centroids[c]\n",
        "                if np.sum((current_centroid-original_centroid)/original_centroid*100.0) > self.tol:\n",
        "                    print(np.sum((current_centroid-original_centroid)/original_centroid*100.0))\n",
        "                    optimized = False\n",
        "\n",
        "            if optimized:\n",
        "                break\n",
        "\n",
        "    def predict(self,data):\n",
        "        distances = [np.linalg.norm(data-self.centroids[centroid]) for centroid in self.centroids]\n",
        "        classification = distances.index(min(distances))\n",
        "        return classification\n",
        "        \n",
        "colors = 10*[\"g\",\"r\",\"c\",\"b\",\"k\"]\n",
        "X = df3.iloc[:,0:2].values\n",
        "clf = K_Means()\n",
        "clf.fit(X)\n",
        "\n",
        "for centroid in clf.centroids:\n",
        "    plt.scatter(clf.centroids[centroid][0], clf.centroids[centroid][1],\n",
        "                marker=\"o\", color=\"k\", s=150, linewidths=5)\n",
        "\n",
        "for classification in clf.classifications:\n",
        "    color = colors[classification]\n",
        "    for featureset in clf.classifications[classification]:\n",
        "        plt.scatter(featureset[0], featureset[1], marker=\"x\", color=color, s=150, linewidths=5)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AaBVxF6RL4CU",
        "colab_type": "text"
      },
      "source": [
        "Problem 4 :Linear Regression from scratch using OOPS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MBIqfDuZAKJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "outputId": "6961780d-39cc-4747-bdaf-07b628b59902"
      },
      "source": [
        "\n",
        "import numpy as np\n",
        "\n",
        "class LinearRegressionModel():\n",
        "\n",
        "    def __init__(self, dataset, learning_rate, num_iterations):\n",
        "        self.dataset = np.array(dataset)\n",
        "        self.b = 0  \n",
        "        self.m = 0  \n",
        "        self.learning_rate = learning_rate\n",
        "        self.num_iterations = num_iterations\n",
        "        self.M = len(self.dataset)\n",
        "        self.tota  l_error = 0\n",
        "\n",
        "    def apply_gradient_descent(self):\n",
        "        for i in range(self.num_iterations):\n",
        "            self.do_gradient_step()\n",
        "\n",
        "    def do_gradient_step(self):\n",
        "        b_summation = 0\n",
        "        m_summation = 0\n",
        "        for i in range(self.M):\n",
        "            x_value = self.dataset[i, 0]\n",
        "            y_value = self.dataset[i, 1]\n",
        "            b_summation += (((self.m * x_value) + self.b) - y_value) \n",
        "            m_summation += (((self.m * x_value) + self.b) - y_value) * x_value\n",
        "        self.b = self.b - (self.learning_rate * (1/self.M) * b_summation)\n",
        "        self.m = self.m - (self.learning_rate * (1/self.M) * m_summation)\n",
        "      \n",
        "    def compute_error(self):\n",
        "        for i in range(self.M):\n",
        "            x_value = self.dataset[i, 0]\n",
        "            y_value = self.dataset[i, 1]\n",
        "            self.total_error += ((self.m * x_value) + self.b) - y_value\n",
        "        return self.total_error\n",
        "\n",
        "    def __str__(self):\n",
        "        return \"Results: b: {}, m: {}, Final Total error: {}\".format(round(self.b, 2), round(self.m, 2), round(self.compute_error(), 2))\n",
        "\n",
        "    def get_prediction_based_on(self, x):\n",
        "        return round(float((self.m * x) + self.b), 2) # Type: Numpy float.\n",
        "\n",
        "def main():\n",
        "    school_dataset = np.genfromtxt(DATASET_PATH, delimiter=\",\")\n",
        "    lr = LinearRegressionModel(school_dataset, 0.0001, 1000)\n",
        "    lr.apply_gradient_descent()\n",
        "    hours = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
        "    for hour in hours:\n",
        "        print(\"Studied {} hours and got {} points.\".format(hour, lr.get_prediction_based_on(hour)))\n",
        "    print(lr)\n",
        "\n",
        "if __name__ == \"__main__\": main()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-0beb326c47a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-14-0beb326c47a5>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0mschool_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenfromtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATASET_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m     \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLinearRegressionModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschool_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mlr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradient_descent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'DATASET_PATH' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2AiZS-eMFRR",
        "colab_type": "text"
      },
      "source": [
        "Logistic Regression using oops"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37AURcbRZBcQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LogisticRegression:\n",
        "  def __init__(self, learning_rate, num_iters, fit_intercept = True, verbose = False):\n",
        "    self.learning_rate = learning_rate\n",
        "    self.num_iters = num_iters\n",
        "    self.fit_intercept = fit_intercept\n",
        "    self.verbose = verbose\n",
        "  def __add_intercept(self, X):\n",
        "    intercept = np.ones((X.shape[0],1))\n",
        "    return np.concatenate((intercept,X),axis=1)\n",
        "  def __sigmoid(self,z):\n",
        "    return 1/(1+np.exp(-z))\n",
        "  def __loss(self, h, y):\n",
        "    return (-y * np.log(h) - (1-y) * np.log(1-h)).mean()\n",
        "  \n",
        "  def fit(self,X,y):\n",
        "    if self.fit_intercept:\n",
        "      X = self.__add_intercept(X)\n",
        "    self.theta = np.zeros(X.shape[1])\n",
        "    \n",
        "    for i in range(self.num_iters):\n",
        "      z = np.dot(X,self.theta)\n",
        "      h = self.__sigmoid(z)\n",
        "      gradient = np.dot(X.T,(h-y))/y.size\n",
        "      \n",
        "      self.theta -= self.learning_rate * gradient\n",
        "      \n",
        "      z = np.dot(X,self.theta)\n",
        "      h = self.__sigmoid(z)\n",
        "      loss = self.__loss(h,y)\n",
        "      \n",
        "      if self.verbose == True and i % 1000 == 0:\n",
        "        print(f'Loss: {loss}\\t')\n",
        "  def predict_probability(self,X):\n",
        "    if self.fit_intercept:\n",
        "      X = self.__add_intercept(X)\n",
        "    return self.__sigmoid(np.dot(X,self.theta))\n",
        "  def predict(self,X):\n",
        "    return (self.predict_probability(X).round())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GlW7KyNOMAcy",
        "colab_type": "text"
      },
      "source": [
        "K means from scratch using oops"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODgvm-8nZFVo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class K_Means:\n",
        "    def __init__(self, k=2, tol=0.001, max_iter=300):\n",
        "        self.k = k\n",
        "        self.tol = tol\n",
        "        self.max_iter = max_iter\n",
        "\n",
        "    def fit(self,data):\n",
        "\n",
        "        self.centroids = {}\n",
        "\n",
        "        for i in range(self.k):\n",
        "            self.centroids[i] = data[i]\n",
        "\n",
        "        for i in range(self.max_iter):\n",
        "            self.classifications = {}\n",
        "\n",
        "            for i in range(self.k):\n",
        "                self.classifications[i] = []\n",
        "\n",
        "            for featureset in X:\n",
        "                distances = [np.linalg.norm(featureset-self.centroids[centroid]) for centroid in self.centroids]\n",
        "                classification = distances.index(min(distances))\n",
        "                self.classifications[classification].append(featureset)\n",
        "\n",
        "            prev_centroids = dict(self.centroids)\n",
        "\n",
        "            for classification in self.classifications:\n",
        "                self.centroids[classification] = np.average(self.classifications[classification],axis=0)\n",
        "\n",
        "            optimized = True\n",
        "\n",
        "            for c in self.centroids:\n",
        "                original_centroid = prev_centroids[c]\n",
        "                current_centroid = self.centroids[c]\n",
        "                if np.sum((current_centroid-original_centroid)/original_centroid*100.0) > self.tol:\n",
        "                    print(np.sum((current_centroid-original_centroid)/original_centroid*100.0))\n",
        "                    optimized = False\n",
        "\n",
        "            if optimized:\n",
        "                break\n",
        "\n",
        "    def predict(self,data):\n",
        "        distances = [np.linalg.norm(data-self.centroids[centroid]) for centroid in self.centroids]\n",
        "        classification = distances.index(min(distances))\n",
        "        return classification\n",
        "        \n",
        "colors = 10*[\"g\",\"r\",\"c\",\"b\",\"k\"]"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}